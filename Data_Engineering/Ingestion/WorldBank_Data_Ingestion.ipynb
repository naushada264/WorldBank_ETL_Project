{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d33a3de0-2852-4ab3-93f4-e919c05e8467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 01. Reading Data from API to spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c83feed-e168-46f6-b226-dfbb7b114178",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Importing All Libraries"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import math\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DoubleType, DateType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cf69f00-6837-4e57-a6ca-a108acf725c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def time_this(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Convert to timezone-aware datetime in UTC\n",
    "        utc_dt = datetime.fromtimestamp(start_time, tz=timezone.utc)\n",
    "\n",
    "        # Convert UTC → IST (UTC+5:30)\n",
    "        ist_dt = utc_dt.astimezone(timezone(timedelta(hours=5, minutes=30)))\n",
    "\n",
    "        # Format as AM/PM\n",
    "        formatted_start_time = ist_dt.strftime(\"%Y-%m-%d %I:%M:%S %p %Z\")\n",
    "\n",
    "        print(f\"⏱️ {func.__name__} started at {formatted_start_time} and executed in {elapsed_time:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "077f9985-80ed-4f63-bee4-f4fe9942bebd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reading API data to a Dictionary"
    }
   },
   "outputs": [],
   "source": [
    "# Thread-local storage for a per-thread Session (connection pooling)\n",
    "_thread_local = threading.local()\n",
    "\n",
    "def get_session():\n",
    "    \"\"\"Return a requests.Session unique to the current thread (reuses TCP connections).\"\"\"\n",
    "    session = getattr(_thread_local, \"session\", None)\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "        retries = Retry(\n",
    "            total=3,\n",
    "            backoff_factor=0.5,\n",
    "            status_forcelist=(429, 500, 502, 503, 504),\n",
    "            allowed_methods=frozenset([\"GET\", \"POST\"])\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retries, pool_connections=1, pool_maxsize=10)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        session.mount(\"http://\", adapter)\n",
    "        _thread_local.session = session\n",
    "    return session\n",
    "\n",
    "def fetch_page(page, top, dataset_id, resource_id, timeout=20):\n",
    "    \"\"\"Fetch a single page using a thread-local session.\"\"\"\n",
    "    base_url = 'https://datacatalogapi.worldbank.org/dexapps/fone/api/apiservice'\n",
    "    skip = top * page\n",
    "    params = {\n",
    "        'datasetId': dataset_id,\n",
    "        'resourceId': resource_id,\n",
    "        'top': top,\n",
    "        'type': 'json',\n",
    "        'skip': skip\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        session = get_session()\n",
    "        resp = session.get(base_url, params=params, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "    except requests.RequestException as e:\n",
    "        # Log and return None so caller can continue\n",
    "        print(f\"[fetch_page] Error fetching page {page}: {e}\")\n",
    "        return None\n",
    "\n",
    "@time_this\n",
    "def fetch_all_data_optimized():\n",
    "    dataset_id = 'DS00975'\n",
    "    resource_id = 'RS00905'\n",
    "    top = 5000\n",
    "\n",
    "    # Step 1: initial request to learn total count\n",
    "    try:\n",
    "        resp = requests.get(\n",
    "            'https://datacatalogapi.worldbank.org/dexapps/fone/api/apiservice',\n",
    "            params={'datasetId': dataset_id, 'resourceId': resource_id, 'top': top, 'type': 'json', 'skip': 0},\n",
    "            timeout=10\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        initial_data = resp.json()\n",
    "        total_count = initial_data.get('count', 0)\n",
    "        all_data = initial_data.get('data', [])\n",
    "        if total_count == 0:\n",
    "            return {'count': 0, 'data': []}\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"[initial request] Error: {e}\")\n",
    "        return {'count': 0, 'data': []}\n",
    "\n",
    "    # compute remaining pages\n",
    "    already = len(all_data)\n",
    "    remaining = max(0, total_count - already)\n",
    "    num_pages_to_fetch = math.ceil(remaining / top)\n",
    "    if num_pages_to_fetch <= 0:\n",
    "        return {'count': total_count, 'data': all_data}\n",
    "\n",
    "    page_numbers = range(1, num_pages_to_fetch + 1)\n",
    "\n",
    "    # Tune max_workers: don't create more threads than pages; keep an upper cap.\n",
    "    max_workers = min(20, max(1, num_pages_to_fetch))\n",
    "    print(f'Total pages to fetch: {num_pages_to_fetch}, using max_workers: {max_workers} and top rows: {top}')\n",
    "\n",
    "    # Step 2: concurrent fetch using as_completed\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_page = {\n",
    "            executor.submit(fetch_page, page, top, dataset_id, resource_id): page\n",
    "            for page in page_numbers\n",
    "        }\n",
    "        print(f'Created {len(future_to_page)} futures.')\n",
    "\n",
    "        for fut in as_completed(future_to_page):\n",
    "            page = future_to_page[fut]\n",
    "            try:\n",
    "                page_result = fut.result(timeout=30)  # optionally set per-future timeout\n",
    "            except Exception as exc:\n",
    "                print(f\"[as_completed] page {page} failed: {exc}\")\n",
    "                continue\n",
    "\n",
    "            if page_result and 'data' in page_result:\n",
    "                # Extend the aggregated list; consider streaming to disk/db if very large\n",
    "                all_data.extend(page_result['data'])\n",
    "\n",
    "    return {'count': total_count, 'data': all_data}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = fetch_all_data_optimized()\n",
    "    #print(f\"Fetched {len(results['data'])} records (API count={results['count']}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61231576-7964-4a2b-bb5e-9728960283f3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Checking Data Size from API"
    }
   },
   "outputs": [],
   "source": [
    "@time_this\n",
    "def data_size_api(results):\n",
    "    return f\"Fetched {len(results['data'])} records (API count={results['count']}).\"\n",
    "\n",
    "print(data_size_api(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a178d88d-9410-4bb7-8ab8-8c0fd87786e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Checking first row"
    }
   },
   "outputs": [],
   "source": [
    "# Checking first row of data\n",
    "#results['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edfc6866-ede0-49a8-b252-0caece6594df",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Converting the Dictionary Data to PySpark Dataframe"
    }
   },
   "outputs": [],
   "source": [
    "@time_this\n",
    "def converting_to_spark_dataframe(results):\n",
    "    # Extract data\n",
    "    data_list = results['data']\n",
    "\n",
    "    # Define the schema\n",
    "    # For numeric columns that might have decimals or be very large, DoubleType is often a safe bet.\n",
    "    # If you need to distinguish between integers and decimals, define separate columns or be precise.\n",
    "    # For amounts, DoubleType or DecimalType are usually appropriate.\n",
    "    schema = StructType([\n",
    "        StructField(\"agreement_signing_date\", StringType(), True),\n",
    "        StructField(\"board_approval_date\", StringType(), True),\n",
    "        StructField(\"borrower\", StringType(), True),\n",
    "        StructField(\"borrowers_obligation\", LongType(), True), # Changed to LongType assuming it should be whole numbers\n",
    "        StructField(\"cancelled_amount\", LongType(), True),     # Changed to LongType\n",
    "        StructField(\"closed_date_most_recent\", StringType(), True),\n",
    "        StructField(\"country\", StringType(), True),\n",
    "        StructField(\"country_code\", StringType(), True),\n",
    "        StructField(\"currency_of_commitment\", StringType(), True), # Assuming None should be treated as string for nulls\n",
    "        StructField(\"disbursed_amount\", DoubleType(), True),   # Changed to DoubleType to accommodate potential decimals or for consistency\n",
    "        StructField(\"due_3rd_party\", LongType(), True),\n",
    "        StructField(\"due_to_ibrd\", LongType(), True),\n",
    "        StructField(\"effective_date_most_recent\", StringType(), True),\n",
    "        StructField(\"end_of_period\", StringType(), True),\n",
    "        StructField(\"exchange_adjustment\", LongType(), True),\n",
    "        StructField(\"first_repayment_date\", StringType(), True),\n",
    "        StructField(\"guarantor\", StringType(), True),\n",
    "        StructField(\"guarantor_country_code\", StringType(), True),\n",
    "        StructField(\"interest_rate\", DoubleType(), True),\n",
    "        StructField(\"last_disbursement_date\", StringType(), True),\n",
    "        StructField(\"last_repayment_date\", StringType(), True),\n",
    "        StructField(\"loan_number\", StringType(), True),\n",
    "        StructField(\"loan_status\", StringType(), True),\n",
    "        StructField(\"loan_type\", StringType(), True),\n",
    "        StructField(\"loans_held\", LongType(), True),\n",
    "        StructField(\"original_principal_amount\", DoubleType(), True), # Changed to DoubleType\n",
    "        StructField(\"project_id\", StringType(), True),\n",
    "        StructField(\"project_name_\", StringType(), True),\n",
    "        StructField(\"region\", StringType(), True),\n",
    "        StructField(\"repaid_3rd_party\", DoubleType(), True),     # Changed to DoubleType\n",
    "        StructField(\"repaid_to_ibrd\", DoubleType(), True),      # Changed to DoubleType\n",
    "        StructField(\"sold_3rd_party\", DoubleType(), True),      # Changed to DoubleType\n",
    "        StructField(\"undisbursed_amount\", DoubleType(), True)  # Changed to DoubleType\n",
    "    ])\n",
    "\n",
    "    # Create DataFrame with defined schema\n",
    "    converted_df = spark.createDataFrame(data_list, schema=schema).withColumn('ingestion_date', F.current_timestamp())\n",
    "\n",
    "    return converted_df\n",
    "\n",
    "api_df = converting_to_spark_dataframe(results)\n",
    "#df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f41acd17-9d64-49c2-8ad0-7b471560350d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Checking Schema"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the schema of dataframe\n",
    "api_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cd2f40a-67c4-4819-afca-8b9616a69294",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Shape Check"
    }
   },
   "outputs": [],
   "source": [
    "# Checking dataframe size\n",
    "@time_this\n",
    "def data_shape(df):\n",
    "    return f'Dataframe has {api_df.count()} rows and {len(api_df.columns)} columns'\n",
    "\n",
    "data_shape(api_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dad3d4b8-a4d9-416e-96b1-123caaad1cae",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Extracting Date Column and converting to Date Type"
    }
   },
   "outputs": [],
   "source": [
    "@time_this\n",
    "def date_conversion(df):\n",
    "    # Extracting Date Columns\n",
    "    # date_columns = []\n",
    "    # for i in df.columns:\n",
    "    #     if \"date\" in i or \"period\" in i:\n",
    "    #         date_columns.append(i)\n",
    "    \n",
    "    date_columns = [c for c in df.columns if \"date\" in c.lower() or \"period\" in c.lower()]\n",
    "\n",
    "    # Changing Datatype\n",
    "    for column in date_columns:\n",
    "        df = df.withColumn(column, F.to_date(F.col(column), \"dd-MMM-yyyy\"))\n",
    "    \n",
    "    df.select(*date_columns).printSchema()\n",
    "\n",
    "    return df\n",
    "\n",
    "api_df = date_conversion(api_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9418726c-457d-4f03-be35-4cb38c134880",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Checing First 5 Rows"
    }
   },
   "outputs": [],
   "source": [
    "@time_this\n",
    "def checking_header(df):\n",
    "    return df.limit(5).display()\n",
    "\n",
    "checking_header(api_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56034f12-8baf-436c-b714-13c5239401d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reading Existing Data and Getting last date of ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710c71b2-36bf-4d2b-8a82-97311848a997",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read Existing Data Schema"
    }
   },
   "outputs": [],
   "source": [
    "df_before_append = spark.read.parquet('/Volumes/worldbank/bronze/ibrd_data_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c05206bc-6f84-47e1-a6f5-d4b569b15aca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check count if all data is scanned"
    }
   },
   "outputs": [],
   "source": [
    "count_before_append = df_before_append.count()\n",
    "count_before_append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49cf3c0f-07eb-40d0-9196-8f8a4aa7d86e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Find the latest partition date to filter the source data"
    }
   },
   "outputs": [],
   "source": [
    "# Find the latest partition value\n",
    "latest_partition_date = df_before_append.agg({\"end_of_period\": \"max\"}).collect()[0][0]\n",
    "print(f\"Latest partition: {latest_partition_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04dbb693-3bf0-46e4-9970-816074cda923",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filter source data for incremental Load"
    }
   },
   "outputs": [],
   "source": [
    "new_data_df = api_df.filter(F.col('end_of_period') > F.lit(latest_partition_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca21fb50-c105-4413-aa4a-0668e7a7838f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "count_new_data = new_data_df.count()\n",
    "print(f'Count of new data -> {count_new_data}')\n",
    "new_data_df.limit(5).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941637e0-4577-4194-a2b1-f82808b459de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write Incremental Data to Existing parquet location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e2f9385-8e4f-4cbe-a37a-0409b707cb18",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Append to existing table incrementally"
    }
   },
   "outputs": [],
   "source": [
    "new_data_df.write.format('parquet').mode('append').partitionBy('end_of_period').save('/Volumes/worldbank/bronze/ibrd_data_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33467e5d-dfd9-4081-895f-d274047b7e36",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Read after Ingetion"
    }
   },
   "outputs": [],
   "source": [
    "df_after_append = spark.read.parquet('/Volumes/worldbank/bronze/ibrd_data_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f74f190-a402-4eb0-af34-e13ba882cdac",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Quality Check"
    }
   },
   "outputs": [],
   "source": [
    "count_after_append = df_after_append.count()\n",
    "print(f'Count before append -> {count_before_append}, Count after append -> {count_after_append}')\n",
    "if count_new_data + count_before_append == count_after_append:\n",
    "    print('Data Load Successful')\n",
    "else:\n",
    "    print('Data Load Not Successful')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "WorldBank_Data_Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
